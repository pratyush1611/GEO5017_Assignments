{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# scikit learn imports for classification functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# plotting imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset\n",
    "\n",
    "From the readme for the xyz files, we know that:\n",
    "\n",
    "Ground truth labels:\n",
    "|File range|Label|\n",
    "|:--|:--|\n",
    "|    000 - 099: |building|\n",
    "|    100 - 199: |car|\n",
    "|    200 - 299: |fence|\n",
    "|    300 - 399: |pole|\n",
    "|    400 - 499: |tree|\n",
    "\n",
    "In following cell: iterate through the files, and collect them in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzPath = './scene_objects/data/*.xyz'\n",
    "dataPathsList = glob.glob(xyzPath)\n",
    "allPointsDF= pd.DataFrame(columns=['x','y','z', 'fileNo', 'groundLabel'])\n",
    "\n",
    "def df_maker(df1, df2):\n",
    "    return pd.concat([df1, df2], sort=False, ignore_index=True)\n",
    "\n",
    "labelToGive = None\n",
    "for path in dataPathsList:\n",
    "    indx = int(path.split('/')[-1][0:3])\n",
    "    # if else to determine label\n",
    "    if indx>=0 and indx<100:\n",
    "        labelToGive = 'building' \n",
    "    elif indx>=100 and indx<200:\n",
    "        labelToGive = 'car' \n",
    "    elif indx>=200 and indx<300:\n",
    "        labelToGive = 'fence' \n",
    "    elif indx>=300 and indx<400:\n",
    "        labelToGive = 'pole' \n",
    "    elif indx>=400 and indx<500:\n",
    "        labelToGive = 'tree' \n",
    "\n",
    "    # using pandas to read dataset and make a dataFrame\n",
    "    tempDF = pd.read_csv(path, delimiter=' ', header=None, dtype=np.float64, names=['x','y','z'])\n",
    "    tempDF.loc[:,'fileNo'] = indx\n",
    "    tempDF.loc[:,'groundLabel'] = labelToGive\n",
    "\n",
    "    # merge with megaDFofPoints\n",
    "    allPointsDF = df_maker(allPointsDF, tempDF)\n",
    "\n",
    "allPointsDF.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making features\n",
    "\n",
    "normalize the feature df <br/>\n",
    "[from stackoverflow we see](https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame), that we can just use pandas for a standard scaling, or else, a [standard scaler from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) can also be applied </br>\n",
    "\n",
    "from [answer here](https://stats.stackexchange.com/questions/417339/data-standardization-vs-normalization-for-clustering-analysis), we see that standard scaler is used for k means , so we are going with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_determiner(indx):\n",
    "    labelToGive=None\n",
    "    if indx>=0 and indx<100:\n",
    "        labelToGive = 'building' \n",
    "    elif indx>=100 and indx<200:\n",
    "        labelToGive = 'car' \n",
    "    elif indx>=200 and indx<300:\n",
    "        labelToGive = 'fence' \n",
    "    elif indx>=300 and indx<400:\n",
    "        labelToGive = 'pole' \n",
    "    elif indx>=400 and indx<500:\n",
    "        labelToGive = 'tree' \n",
    "    return labelToGive\n",
    "\n",
    "\n",
    "featureDF = allPointsDF.groupby('fileNo').var()\n",
    "featureDF.rename(columns={'x':'varX','y':'varY','z':'varZ'}, inplace=True)\n",
    "featureDF.loc[:,'median_Z'] = allPointsDF.groupby('fileNo').z.median()\n",
    "# featureDF.loc[:,'mean_Z'] = allPointsDF.groupby('fileNo').z.mean()\n",
    "\n",
    "# range of x,y,z\n",
    "featureDF.loc[:,'range_X'] = allPointsDF.groupby('fileNo').x.max() - allPointsDF.groupby('fileNo').x.min()\n",
    "featureDF.loc[:,'range_Y'] = allPointsDF.groupby('fileNo').y.max() - allPointsDF.groupby('fileNo').y.min()\n",
    "featureDF.loc[:,'range_Z'] = allPointsDF.groupby('fileNo').z.max() - allPointsDF.groupby('fileNo').z.min()\n",
    "\n",
    "featureDF.loc[:,'Volume'] = allPointsDF.set_index('fileNo').loc[:,'x':'z'].groupby('fileNo').apply(ConvexHull).apply(lambda x: x.volume)\n",
    "\n",
    "# points density\n",
    "featureDF.loc[:,'footprintDensity'] =  allPointsDF.groupby('fileNo').count().x / (featureDF.range_X * featureDF.range_Y)\n",
    "featureDF.loc[:,'volumeDensity'] =  allPointsDF.groupby('fileNo').count().x / featureDF.Volume\n",
    "\n",
    "featureDF.loc[:,'label'] = featureDF.reset_index().fileNo.apply(label_determiner)\n",
    "\n",
    "noLabelFeatureDF = featureDF.iloc[:,:-1].copy()\n",
    "\n",
    "# standardize DF\n",
    "standardFeatureDF = (noLabelFeatureDF - noLabelFeatureDF.mean() ) / noLabelFeatureDF.std()\n",
    "standardFeatureDF = standardFeatureDF.join(other=featureDF.label , on='fileNo') # join labels to the DF\n",
    "\n",
    "# normalize df using min max scaling\n",
    "minMaxFeatureDF = (noLabelFeatureDF- noLabelFeatureDF.min()) / (noLabelFeatureDF.max() - noLabelFeatureDF.min())\n",
    "minMaxFeatureDF = minMaxFeatureDF.join(other=featureDF.label , on='fileNo') # join labels to the DF\n",
    "\n",
    "# featureDF.to_pickle('./scene_objects/featureData.pkl')\n",
    "# standardFeatureDF.to_pickle('./scene_objects/standardFeatureData.pkl')\n",
    "# minMaxFeatureDFv.to_pickle('./scene_objects/minMaxFeatureDF.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting to see resemblamces and clusters, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load df's\n",
    "\n",
    "# featureDF = pd.read_pickle('./scene_objects/featureData.pkl')\n",
    "# standardFeatureDF = pd.read_pickle('./scene_objects/standardFeatureData.pkl')\n",
    "# minMaxFeatureDFv = pd.read_pickle('./scene_objects/minMaxFeatureDF.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=featureDF, hue=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=standardFeatureDF, hue=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=minMaxFeatureDF, hue=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Split dataset to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTrainSplitter(DataFrame, testSize=0.3 ,randomState=45 ):\n",
    "    \"\"\"\n",
    "    Summary: takes a dataframe in the format of the one defined in section making features, and returns test set train set X,y\n",
    "    ===================\n",
    "    Arguments:\n",
    "        DataFrame (pd.DataFrame):\n",
    "        testSize (float):\n",
    "        randomState (int):\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    X = DataFrame.drop('label', axis=1)\n",
    "    y = DataFrame.loc[:,'label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=testSize, random_state=randomState , shuffle=True)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "def predictionAccuracyChecker(predictedList, y_testSeries):\n",
    "    \"\"\"\n",
    "    Summary: function to check the prediction vs actual labels\n",
    "    Args:\n",
    "        predictedList (list): list obtained from classifier\n",
    "        y_testSeries (pd.Series) : pd series obtained as y_test from train_test_split\n",
    "    \"\"\"\n",
    "    y_testList = y_testSeries.tolist()\n",
    "    if len(predictedList) != len(y_testList):\n",
    "        return \"error, not same length\"\n",
    "\n",
    "    truePredict = 0\n",
    "    falsePredict = 0\n",
    "    for i,j in zip(predictedList, y_testList):\n",
    "        if i==j:\n",
    "            truePredict+=1\n",
    "        else:\n",
    "            falsePredict+=1\n",
    "    return truePredict/len(predictedList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "minMax_X_train, minMax_X_test, minMax_y_train, minMax_y_test = testTrainSplitter(minMaxFeatureDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmClassifier = SVC(decision_function_shape='ovo', kernel='linear')\n",
    "svmClassifier.fit(minMax_X_train, minMax_y_train)\n",
    "\n",
    "predList = svmClassifier.predict(minMax_X_test)\n",
    "predictionAccuracyChecker( predList , minMax_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8266666666666667"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfClassifier = RandomForestClassifier(n_estimators = 50, max_depth=2, random_state=0)\n",
    "rfClassifier.fit(minMax_X_train, minMax_y_train)\n",
    "predList = rfClassifier.predict(minMax_X_test)\n",
    "predictionAccuracyChecker( predList , minMax_y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63416c4ef3d01407afb7ae6f8b52f5ba040582e27e37581275fb0a6850709428"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('simNvis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
